{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmanDeep-pvt/Self-Projects/blob/main/Movie_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9341f20c",
        "outputId": "f86efe50-761c-4cd1-de53-e555aae49837"
      },
      "source": [
        "!pip install numpy<2\n",
        "!pip install surprise"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2: No such file or directory\n",
            "Requirement already satisfied: surprise in /usr/local/lib/python3.12/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.12/dist-packages (from surprise) (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ln6kFSTitj32",
        "outputId": "4c6542db-5d4b-43da-b3dc-1f84191a6731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2202303174.py\", line 10, in <cell line: 0>\n",
            "    from surprise import Reader, Dataset, SVD\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\", line 6, in <module>\n",
            "    from .prediction_algorithms import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n",
            "    from .algo_base import AlgoBase\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n",
            "    from .. import similarities as sims\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2202303174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .prediction_algorithms import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malgo_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbaseline_only\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mco_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaseline_als\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPredictionImpossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/similarities.pyx\u001b[0m in \u001b[0;36minit surprise.similarities\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# movie_recommender.py\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "# Load the MovieLens dataset (using the small version)\n",
        "# Ensure you have 'ratings.csv' and 'movies.csv' in the same directory\n",
        "try:\n",
        "    ratings = pd.read_csv('ratings.csv')\n",
        "    movies = pd.read_csv('movies.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset files not found. Please download 'ratings.csv' and 'movies.csv' from MovieLens.\")\n",
        "    # exit() # Removed exit() to allow the rest of the code to be modified if needed\n",
        "\n",
        "# Merge ratings and movies dataframes\n",
        "if 'ratings' in locals() and 'movies' in locals():\n",
        "    df = pd.merge(ratings, movies, on='movieId')\n",
        "\n",
        "    # --- 2. Content-Based Filtering ---\n",
        "\n",
        "    # This approach recommends movies similar to the ones a user has liked before.\n",
        "    # We will use movie genres to determine similarity.\n",
        "\n",
        "    # Create a TF-IDF Vectorizer Object. Remove all english stop words\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Replace NaN with an empty string in the 'genres' column\n",
        "    movies['genres'] = movies['genres'].fillna('')\n",
        "\n",
        "    # Construct the required TF-IDF matrix by fitting and transforming the data\n",
        "    tfidf_matrix = tfidf.fit_transform(movies['genres'])\n",
        "\n",
        "    # Compute the cosine similarity matrix\n",
        "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    # Construct a reverse map of indices and movie titles\n",
        "    indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n",
        "\n",
        "    def get_content_based_recommendations(title, cosine_sim=cosine_sim, top_n=5):\n",
        "        \"\"\"\n",
        "        Get top N movie recommendations based on content similarity.\n",
        "        \"\"\"\n",
        "        if title not in indices:\n",
        "            return f\"Movie '{title}' not found in the dataset.\"\n",
        "\n",
        "        # Get the index of the movie that matches the title\n",
        "        idx = indices[title]\n",
        "\n",
        "        # Get the pairwise similarity scores of all movies with that movie\n",
        "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "        # Sort the movies based on the similarity scores\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Get the scores of the top_n most similar movies\n",
        "        sim_scores = sim_scores[1:top_n+1]\n",
        "\n",
        "        # Get the movie indices\n",
        "        movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "        # Return the top_n most similar movies\n",
        "        return movies['title'].iloc[movie_indices]\n",
        "\n",
        "    # --- 3. Collaborative Filtering ---\n",
        "\n",
        "    # This approach uses the user-item rating matrix to find similar users or items.\n",
        "    # We will use the Surprise library with the SVD algorithm.\n",
        "\n",
        "    # The Reader class is used to parse a file containing ratings.\n",
        "    reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "    # The columns must correspond to user id, item id and ratings (in that order).\n",
        "    data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "    # We'll use the famous SVD algorithm.\n",
        "    algo = SVD()\n",
        "\n",
        "    # Train the algorithm on the full dataset\n",
        "    trainset = data.build_full_trainset()\n",
        "    algo.fit(trainset)\n",
        "\n",
        "    def get_collaborative_filtering_recommendations(movie_title, top_n=5):\n",
        "        \"\"\"\n",
        "        Get top N movie recommendations using collaborative filtering (SVD).\n",
        "        \"\"\"\n",
        "        if movie_title not in movies['title'].values:\n",
        "            return f\"Movie '{movie_title}' not found in the dataset.\"\n",
        "\n",
        "        # Get the movie ID for the given title\n",
        "        movie_id = movies.loc[movies['title'] == movie_title, 'movieId'].iloc[0]\n",
        "\n",
        "        # Get a list of all movie IDs\n",
        "        all_movie_ids = ratings['movieId'].unique()\n",
        "\n",
        "        # Predict ratings for all movies for a hypothetical user who likes the input movie.\n",
        "        # We can create a \"test user\" who has rated the input movie highly.\n",
        "        # Here, we simplify by predicting ratings for movies this user hasn't seen.\n",
        "\n",
        "        # Get movie IDs that are not the input movie\n",
        "        movies_to_predict = [mid for mid in all_movie_ids if mid != movie_id]\n",
        "\n",
        "        # Create a dummy user ID\n",
        "        dummy_user_id = 9999\n",
        "\n",
        "        # Predict ratings for all other movies\n",
        "        predictions = [algo.predict(dummy_user_id, movie_id_to_predict, 4) for movie_id_to_predict in movies_to_predict]\n",
        "\n",
        "        # Sort predictions by estimated rating\n",
        "        predictions.sort(key=lambda x: x.est, reverse=True)\n",
        "\n",
        "        # Get top N recommendations\n",
        "        top_movie_ids = [pred.iid for pred in predictions[:top_n]]\n",
        "\n",
        "        # Get movie titles from IDs\n",
        "        recommended_movies = movies[movies['movieId'].isin(top_movie_ids)]['title']\n",
        "\n",
        "        return recommended_movies\n",
        "\n",
        "    # --- 4. Visualization ---\n",
        "\n",
        "    def visualize_ratings_distribution():\n",
        "        \"\"\"\n",
        "        Visualize the distribution of user ratings.\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.countplot(x='rating', data=ratings)\n",
        "        plt.title('Distribution of Movie Ratings', fontsize=15)\n",
        "        plt.xlabel('Rating')\n",
        "        plt.ylabel('Count')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_recommendations(movie_title, recommendations, method):\n",
        "        \"\"\"\n",
        "        Visualize the recommendations for a given movie.\n",
        "        \"\"\"\n",
        "        if isinstance(recommendations, str):\n",
        "            print(recommendations)\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        # Using a placeholder series for similarity scores as we don't have them directly for collaborative filtering\n",
        "        scores = pd.Series(range(len(recommendations), 0, -1), index=recommendations.index)\n",
        "\n",
        "        sns.barplot(x=scores.values, y=recommendations.values, palette='viridis')\n",
        "        plt.title(f'Top 5 {method} Recommendations for \"{movie_title}\"', fontsize=14)\n",
        "        plt.xlabel('Recommendation Strength (Illustrative)')\n",
        "        plt.ylabel('Movie Title')\n",
        "        plt.show()\n",
        "\n",
        "    # --- 5. Main Execution ---\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        # --- Demonstrate the Recommender System ---\n",
        "\n",
        "        # Pick a movie to get recommendations for\n",
        "        target_movie = 'Forrest Gump (1994)'\n",
        "\n",
        "        print(f\"Movie Recommendation System\\n\")\n",
        "        print(f\"Target Movie: {target_movie}\\n\")\n",
        "\n",
        "        # Get and print content-based recommendations\n",
        "        print(\"--- Content-Based Recommendations ---\")\n",
        "        content_recs = get_content_based_recommendations(target_movie)\n",
        "        if isinstance(content_recs, str):\n",
        "            print(content_recs)\n",
        "        else:\n",
        "            print(content_recs.to_string(index=False))\n",
        "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "        # Get and print collaborative filtering recommendations\n",
        "        print(\"--- Collaborative Filtering Recommendations ---\")\n",
        "        collab_recs = get_collaborative_filtering_recommendations(target_movie)\n",
        "        if isinstance(collab_recs, str):\n",
        "            print(collab_recs)\n",
        "        else:\n",
        "            print(collab_recs.to_string(index=False))\n",
        "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "        # --- Generate Visualizations ---\n",
        "\n",
        "        # Visualize the overall rating distribution\n",
        "        print(\"Displaying visualization for overall movie ratings distribution...\")\n",
        "        visualize_ratings_distribution()\n",
        "\n",
        "        # Visualize recommendations from both methods\n",
        "        print(f\"Displaying visualization for Content-Based recommendations for '{target_movie}'...\")\n",
        "        visualize_recommendations(target_movie, content_recs, \"Content-Based\")\n",
        "\n",
        "        print(f\"Displaying visualization for Collaborative Filtering recommendations for '{target_movie}'...\")\n",
        "        visualize_recommendations(target_movie, collab_recs, \"Collaborative Filtering\")"
      ]
    }
  ]
}